+++
authors = ["Javier Orraca",]
title = "The Hitchhiker's Guide to PySpark"
date = "2019-07-05"
tags = ["blog",]
images = ["PySpark.jpg",]
+++

Rahul Agarwal recently wrote an article for Towards Data Science that provides a great overview on the history of MapReduce, Spark, and using PySpark for data engineering with Python.
<!--more-->
I worked on a project recently with two people and we were shocked at how computationally efficient Spark was for our use case. We were able to load, read, manipulate, and explore over 30 GB of data in a Google Colab, GPU-enabled environment _without_ reaching Google's RAM limitation... If you've used Colab, you know that you can eat through 12 GB of GPU RAM _quickly_. I highly recommend this article for you if you're unfamiliar with Spark and PySpark for Python.

Source:

* [The Hitchhiker's Guide to Handle Big Data Using Spark](https://towardsdatascience.com/the-hitchhikers-guide-to-handle-big-data-using-spark)